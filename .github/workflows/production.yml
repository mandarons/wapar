name: Production Deployment

on:
  push:
    branches: [main]
  workflow_dispatch:
  schedule: 
    - cron: "0 * * * *"

jobs:
  test-backend:
    name: Test Backend - ${{ matrix.test-type }}
    runs-on: ubuntu-latest
    strategy:
      matrix:
        test-type: [unit, integration]
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Bun
        uses: oven-sh/setup-bun@v2
        with:
          bun-version: latest

      - name: Cache server dependencies
        uses: actions/cache@v4
        id: cache-server
        with:
          path: server/node_modules
          key: ${{ runner.os }}-bun-server-${{ hashFiles('server/bun.lock') }}
          restore-keys: |
            ${{ runner.os }}-bun-server-

      - name: Install server dependencies
        if: steps.cache-server.outputs.cache-hit != 'true'
        run: cd server && bun install --frozen-lockfile

      - name: Run server ${{ matrix.test-type }} tests
        run: cd server && bun run test:${{ matrix.test-type }}

  test-backend-coverage:
    name: Test Backend Coverage
    runs-on: ubuntu-latest
    needs: test-backend
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Bun
        uses: oven-sh/setup-bun@v2
        with:
          bun-version: latest

      - name: Install lcov (for HTML report generation)
        run: sudo apt-get update && sudo apt-get install -y lcov

      - name: Cache server dependencies
        uses: actions/cache@v4
        id: cache-server
        with:
          path: server/node_modules
          key: ${{ runner.os }}-bun-server-${{ hashFiles('server/bun.lock') }}
          restore-keys: |
            ${{ runner.os }}-bun-server-

      - name: Install server dependencies
        if: steps.cache-server.outputs.cache-hit != 'true'
        run: cd server && bun install --frozen-lockfile

      - name: Run tests with coverage
        run: cd server && bun test --coverage

      - name: Generate HTML coverage report
        run: cd server && bun run test:coverage:html

      - name: Upload coverage report
        uses: actions/upload-artifact@v4
        with:
          name: coverage-report
          path: server/coverage/html/
          retention-days: 90

      - name: Check coverage threshold (must be 100%)
        run: |
          cd server
          COVERAGE=$(bun test --coverage 2>&1 | grep "All files" | awk '{print $4}' | sed 's/%//')
          echo "Coverage: ${COVERAGE}%"
          if (( $(echo "$COVERAGE < 100" | bc -l) )); then
            echo "❌ Coverage is below 100% (current: ${COVERAGE}%)"
            exit 1
          fi
          echo "✅ Coverage is at 100%"

  test-frontend:
    name: Test Frontend - ${{ matrix.test-type }}
    runs-on: ubuntu-latest
    strategy:
      matrix:
        test-type: [unit, e2e]
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Bun
        uses: oven-sh/setup-bun@v2
        with:
          bun-version: latest

      - name: Cache frontend dependencies
        uses: actions/cache@v4
        id: cache-app
        with:
          path: app/node_modules
          key: ${{ runner.os }}-bun-app-${{ hashFiles('app/bun.lock') }}
          restore-keys: |
            ${{ runner.os }}-bun-app-

      - name: Install frontend dependencies
        if: steps.cache-app.outputs.cache-hit != 'true'
        run: cd app && bun install --frozen-lockfile

      - name: Cache Playwright browsers
        if: matrix.test-type == 'e2e'
        uses: actions/cache@v4
        id: playwright-cache
        with:
          path: ~/.cache/ms-playwright
          key: ${{ runner.os }}-playwright-${{ hashFiles('app/bun.lock') }}
          restore-keys: |
            ${{ runner.os }}-playwright-

      - name: Install Playwright browsers
        if: matrix.test-type == 'e2e' && steps.playwright-cache.outputs.cache-hit != 'true'
        run: cd app && bunx playwright install --with-deps chromium

      - name: Run frontend unit tests
        if: matrix.test-type == 'unit'
        run: cd app && bun run test:unit --run --reporter=dot

      - name: Run frontend e2e tests
        if: matrix.test-type == 'e2e'
        run: cd app && bun run test:e2e

  deploy-backend:
    name: Verify Backend Build for Production
    runs-on: ubuntu-latest
    needs: [test-backend, test-backend-coverage]
    permissions:
      contents: read
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Bun
        uses: oven-sh/setup-bun@v2
        with:
          bun-version: latest

      - name: Cache server dependencies
        uses: actions/cache@v4
        with:
          path: server/node_modules
          key: ${{ runner.os }}-bun-server-${{ hashFiles('server/bun.lock') }}
          restore-keys: |
            ${{ runner.os }}-bun-server-

      - name: Install server dependencies
        run: cd server && bun install --frozen-lockfile

      - name: Verify database migrations are ready
        run: cd server && bun run db:generate --check

      - name: Build verification
        run: |
          cd server
          echo "✅ Server is ready for production deployment"
          echo "Note: Server runs locally with SQLite - no cloud deployment needed"
          echo "To deploy: Use ./run.sh on your production server"
          echo "Ensure proper backups and monitoring are in place"

  deploy-frontend:
    name: Deploy Frontend to Production
    runs-on: ubuntu-latest
    needs: test-frontend
    permissions:
      contents: read
      deployments: write
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Bun
        uses: oven-sh/setup-bun@v2
        with:
          bun-version: latest

      - name: Cache frontend dependencies
        uses: actions/cache@v4
        with:
          path: app/node_modules
          key: ${{ runner.os }}-bun-app-${{ hashFiles('app/bun.lock') }}
          restore-keys: |
            ${{ runner.os }}-bun-app-

      - name: Install frontend dependencies (if needed)
        run: cd app && (bun install --frozen-lockfile || true)

      - name: Build frontend
        run: cd app && bun run build
        env:
          PUBLIC_URL: "."
          PUBLIC_API_URL: https://wapar-api.mandarons.com

      - name: Deploy Pages to production
        uses: cloudflare/wrangler-action@v3
        with:
          apiToken: ${{ secrets.CLOUDFLARE_API_TOKEN }}
          accountId: ${{ secrets.CLOUDFLARE_ACCOUNT_ID }}
          command: pages deploy app/.svelte-kit/cloudflare --project-name=wapar-frontend --branch=main --commit-dirty=true

  performance-tests:
    name: Performance Measurement
    runs-on: ubuntu-latest
    needs: [deploy-backend, deploy-frontend]
    permissions:
      contents: read
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: '20'

      - name: Wait for production deployment to be ready
        run: |
          echo "Waiting for production API to be ready..."
          for i in {1..30}; do
            if curl -f -s https://wapar-api.mandarons.com/api > /dev/null; then
              echo "Production API is ready!"
              break
            fi
            echo "Attempt $i: API not ready yet, waiting..."
            sleep 10
          done

      - name: Run performance tests
        run: node scripts/performance-test.js
        env:
          ENVIRONMENT: production
          API_URL: https://wapar-api.mandarons.com
          FRONTEND_URL: https://wapar.mandarons.com
          NUM_REQUESTS: '20'
          WARMUP_REQUESTS: '3'
          OUTPUT_DIR: ./performance-results

      - name: Upload performance results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: production-performance-results
          path: performance-results/
          retention-days: 30

      - name: Display performance summary
        if: always()
        run: |
          if [ -f performance-results/performance-report.md ]; then
            echo "## Performance Test Results" >> $GITHUB_STEP_SUMMARY
            cat performance-results/performance-report.md >> $GITHUB_STEP_SUMMARY
          fi
